---
output: github_document
bibliography: references.bib
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
knitr::opts_chunk$set(fig.width=10, fig.height=6, fig.path="README_figures/")
```

[![Build Status](https://travis-ci.org/BenjaK/scanstatistics.svg?branch=master)](https://travis-ci.org/BenjaK/scanstatistics)
<!--
[![Build status](https://ci.appveyor.com/api/projects/status/tbd7gs7n2aaa1yvf/branch/master?svg=true)](https://ci.appveyor.com/project/BenjaK/scanstatistics/branch/master)
[![Coverage Status](https://coveralls.io/repos/github/BenjaK/scanstatistics/badge.svg?branch=master)](https://coveralls.io/github/BenjaK/scanstatistics?branch=master)
-->

# scanstatistics
An R package for space-time anomaly detection using scan statistics.

## Installing the package

To install the development version of this package, type the following:
```{r install, eval=FALSE}
devtools::install_github("benjak/scanstatistics")
```

## What are scan statistics?
Scan statistics are used to detect anomalous clusters in spatial or space-time 
data. The gist of the methodology, at least in this package, is this:

1. Monitor one or more data streams at multiple _locations_ over intervals of 
   time. 
2. Form a set of space-time _clusters_, each consisting of (1) a collection of 
   locations, and (2) an interval of time stretching from the present to some 
   number of time periods in the past.
3. For each cluster, compute a statistic based on both the observed and the 
   expected responses. Report the clusters with the largest statistics.
   
## Main functions

- __`scan_poisson`__: computes a scan statistic for data following a Poisson
                      distribution.
- __`scan_negbin`__: computes a scan statistic for data following a negative 
                     binomial distribution.
- __`scan_zip`__: computes a scan statistic for data following a zero-inflated 
                  Poisson distribution.
- __`knn_zones`__: Creates a set of spatial _zones_ (groups of locations) to
                   scan for anomalies. Input is a matrix in which rows are the
                   enumerated locations, and columns the $k$ nearest neighbors.
                   To create such a matrix, the following two functions are
                   useful:
    + __`coords_to_knn`__: use `stats::dist` to get the $k$ nearest neighbors
                           of each location into a format usable by `knn_zones`.
    + __`dist_to_knn`__: use an already computed distance matrix to get the
                         $k$ nearest neighbors of each location into a format 
                         usable by `knn_zones`.
- __`score_locations`__: Score each location by how likely it is to have an 
                         ongoing anomaly in it. This score is not theoretically
                         motivated.
- __`top_clusters`__: Get the top $k$ space-time clusters, either overlapping or
                      non-overlapping in the spatial dimension.
                      
## Example: Brain cancer in New Mexico
To demonstrate the scan statistics in this package, we will use a dataset of the 
annual number of brain cancer cases in the counties of New Mexico, for the years 
1973--1991. This data was studied by @Kulldorff1998, who detected a probable 
cluster of cancer cases in the counties Los Alamos and Santa Fe during the years 
1986--1989. The data originally comes from the package _rsatscan_ [@rsatscan], 
which provides an interface to the program [SatScan](http://www.satscan.org), 
but has been aggregated and extended for the _scanstatistics_ package.

The counties of New Mexico may be plotted on a map as follows:
```{r, cache=TRUE}
library(scanstatistics)
library(ggplot2)

# Load map data
data(NM_map)

# Place the county names at the centroid of the counties. The following function
# calculates the geographical centroids from the polygons in NM_map.
# See: https://en.wikipedia.org/wiki/Centroid#Centroid_of_polygon
polygon_centroid <- function(x0, y0) {
  x1 <- c(x0[-1], x0[1])
  y1 <- c(y0[-1], y0[1])
  A <- sum(x0 * y1 - x1 * y0) / 2
  Cx <- sum((x0 + x1) * (x0 * y1 - x1 * y0)) / (6 * A)
  Cy <- sum((y0 + y1) * (x0 * y1 - x1 * y0)) / (6 * A)
  data.frame(long = Cx, lat = Cy)
}

# Calculate geographic centroids for each county
centroids <- NM_map[, polygon_centroid(long, lat), by = .(subregion)]

# Plot map with labels at centroids
ggplot() + 
  geom_polygon(data = NM_map,
               mapping = aes(x = long, y = lat, group = group),
               color = "grey", fill = "white") +
  geom_text(data = centroids, 
            mapping = aes(x = long, y = lat, label = subregion)) +
  ggtitle("Counties of New Mexico")
```

### Creating spatial zones
The anomalies considered in the _scanstatistics_ package have both a temporal 
and a spatial component. The spatial component, called a zone, consists of one 
or more locations grouped together according to their similarity across 
features. In this example, the locations are the counties of New Mexico and the
features are the coordinates of the county seats. These are made available in 
the data table `NM_geo`. Similarity will be measured using the geographical 
distance between the seats of the counties, taking into account the curvature
of the earth. A distance matrix is calculated using the `sp_Dists` function from 
the _sp_ package, which is then passed to `dist_to_knn` (with $k=15$ neighbors) 
and on to `knn_zones`:
```{r, cache=TRUE}
library(sp)
library(magrittr)

data(NM_geo)

zones <- NM_geo[, c("long", "lat"), with = FALSE] %>%
  as.matrix %>%
  spDists(x = ., y = ., longlat = TRUE) %>%
  dist_to_knn(k = 15) %>%
  knn_zones
```

### A scan statistic for Poisson data
The Poisson distribution is a natural first option when dealing with 
(practically) unbounded count data. The _scanstatistics_ package provides the 
function `scan_poisson`, which is a scan statistic for univariate 
Poisson-distributed data proposed by @Neill2005.

The first argument to `scan_poisson` should be a __data table__ with columns
'location', 'duration', 'count' and 'mu'. This table holds data for the period
in which we want to detect anomalies. Locations should be encoded as the 
integers 1, 2, ..., which means that factor variables can be used for this 
purpose. The duration column counts time backwards, so that a duration of 1 is 
the most recent time interval, duration 2 is the second most recent, and so on. 

We will create such a table by subsetting the `NM_popcas` table, which holds the
population and the number of brain cancer cases for each year between 
$1973--1991$ and each county of New Mexico. Note that the population numbers are
(perhaps poorly) interpolated from the censuses conducted in 1973, 1982, and 
1991.
```{r, cache=TRUE}
data(NM_popcas)

tab <- NM_popcas[year >= 1986 & year < 1990, ]
tab[, duration := max(year) - year + 1]
tab[, location := county]
```

We still need to add the column 'mu', which should hold the predicted Poisson
expected value parameter $\mu_{it}$ for each location $i$ and time interval $t$.
In this example we would like to detect a potential cluster of brain cancer in 
the counties of New Mexico during the years 1986--1989. Thus, we will use data 
from the years prior to 1986 to estimate the Poisson parameter for all counties 
in the years following. A simple generalized linear model (GLM) with a linear 
time trend and an offset for county population size will suffice to demonstrate 
the scan statistic. We fit such a model and create the needed column as follows:

```{r, cache=TRUE}
# Fit model and add predicted expected value parameters
mod_poisson <- glm(count ~ offset(log(population)) + 1 + I(year - 1985), 
                   data = NM_popcas[year < 1986, ], 
                   family = poisson(link = "log"))

# Add the expected value parameter column
tab[, mu := predict(mod_poisson, tab, type = "response")]
```

We can now calculate the Poisson scan statistic. To give us more confidence 
in our detection results, we will perform 99 Monte Carlo replications, by which 
data is generated using the parameters from the null hypothesis and a new scan
statistic calculated. This is then summarized in a $p$-value, calculated as
the proportion of times the replicated scan statistics exceeded the observed 
one. The output of `scan_poisson` is an object of class "scanstatistic", which
comes with the print method seen below.

```{r, cache=TRUE}
set.seed(1)
poisson_result <- scan_poisson(tab, zones, n_mcsim = 99)
print(poisson_result)
```

As we can see, the most likely cluster for an anomaly stretches from 1986--1989
and involves the locations numbered 15 and 26, which correspond to the counties
```{r, comment=NA, cache=TRUE}
counties <- as.character(NM_geo$county)
counties[c(15, 26)]
```

These are the same counties detected by @Kulldorff1998, though their analysis
was retrospective rather than prospective as ours was. Ours was also data 
dredging (adjective) as we used the same study period with hopes of detecting
the same cluster.

#### A heuristic score for locations
We can score each county according to how likely it is to be part of a cluster
in a heuristic fashion using the function `score_locations`, and visualize the
results on a heatmap as follows:
```{r, cache=TRUE}
# Calculate scores and add column with county names
county_scores <- score_locations(poisson_result)
county_scores[, county := counties]

# Create a table for plotting
score_map_df <- merge(NM_map, county_scores, by = "county", all.x = TRUE)
score_map_df[subregion == "cibola", 
             relative_score := county_scores[county == "valencia", relative_score]]

ggplot() + 
  geom_polygon(data = score_map_df,
               mapping = aes(x = long, y = lat, group = group, 
                             fill = relative_score),
               color = "grey") +
  scale_fill_gradient(low = "#e5f5f9", high = "darkgreen",
                      guide = guide_colorbar(title = "Relative\nScore")) +
  geom_text(data = centroids, 
            mapping = aes(x = long, y = lat, label = subregion),
            alpha = 0.5) +
  ggtitle("County scores")
```

As we can see, this does not match up entirely with the previous results as 
Torrance was not part of the most likely cluster.

#### Finding the top-scoring clusters
Finally, if we want to know not just the most likely cluster, but say the five
top-scoring space-time clusters, we can use the function `top_clusters`.
The clusters returned can either be overlapping or non-overlapping in the 
spatial dimension, according to our liking.
```{r, cache=TRUE}
library(dplyr)

top5 <- top_clusters(poisson_result, k = 5, overlapping = FALSE)

# Add the counties corresponding to the zone as a column
top_counties <- top5$zone %>%
  purrr::map(get_zone, zones = zones) %>%
  purrr::map(function(x) counties[x])
top5[, counties := top_counties]
```

To get $p$-values for these clusters, the values of the cluster-specific 
statistics in the table above can be compared to the replicate scan statistics
calculated earlier. These $p$-values will be conservative, since secondary 
clusters from the original data are compared to the most likely clusters from
the replicate data sets.
```{r, cache=TRUE}
top5[, pvalue := mc_pvalue(statistic, poisson_result$replicated)]

top5
```

### A scan statistic for negative binomial data
For count data with overdispersion, the _scanstatistics_ package provides the 
function `scan_negbin`, which is the expectation-based scan statistic invented 
by @Tango2011. This scan statistic assumes that the data follows a negative 
binomial distribution parametrized by its expected value $\mu$ and a parameter 
$\theta$ such that a count $Y$ has variance 
$\text{Var}(Y) = \mu + \mu^2 / \theta$. The parameters $\mu$ and $\theta$ may
vary over both location and time.

### A scan statistic for zero-inflated Poisson data

# References
